{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "caoe",
   "display_name": "caoe",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sklearn\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "import socialSig\n",
    "importlib.reload(socialSig)\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   sending_citizen_unspecified  sending_citizenship_unknown  \\\n0                  1134.995083                   243.827586   \n1                   917.067746                   600.000000   \n2                   637.977315                   557.150000   \n3                  1178.378744                  1678.562500   \n4                  2231.128863                  2916.538705   \n\n   sending_household_not_owned  sending_household_owned  \\\n0                  1178.209016              1106.832815   \n1                  1274.160656               795.118020   \n2                   729.454795               610.791414   \n3                  1560.244328              1084.109969   \n4                  2104.943216              2278.834178   \n\n   sending_household_owned_unknown  sending_indigeneity  sending_internet  \\\n0                       736.562500           918.960526       2111.233685   \n1                        -1.000000            -1.000000       1988.103175   \n2                       468.392857           584.454545       1310.462428   \n3                       785.742857          1165.943463       2109.431851   \n4                      2353.519005          2527.268790       3075.771459   \n\n   sending_internet_unknown  sending_marriage_unknown  sending_married  ...  \\\n0                347.698113                576.362069      1921.620722  ...   \n1                459.166667                342.800000      1383.286109  ...   \n2                514.312500               3800.000000      1079.759162  ...   \n3               1043.500000                549.695652      1945.953046  ...   \n4               1941.578834                650.206897      3749.195659  ...   \n\n   sending_unknown_employment_status  sending_unknown_indigeneity  \\\n0                        1218.357143                   840.339623   \n1                         535.750000                   576.379310   \n2                         540.000000                   629.215385   \n3                         771.400000                   694.579439   \n4                        1531.622086                  2064.311411   \n\n   sending_unpaid_worker  sending_urban  sending_weighted_avg_income  \\\n0                      0    1597.239059                  1124.200953   \n1                      0      -1.000000                   908.782316   \n2                      0      -1.000000                   628.585520   \n3                      0    1436.934837                  1173.751889   \n4                      0    2319.113608                  2230.022635   \n\n   sending_weighted_avg_income_abroad  sending_weighted_avg_no_income_abroad  \\\n0                          768.788518                            1213.505104   \n1                          720.767123                             929.317603   \n2                          486.116034                             738.533015   \n3                          861.385645                            1219.547797   \n4                         1672.340116                            2273.630651   \n\n   sending_weighted_avg_unknown_income_abroad  sending_widowed  US_MIG_05_10  \n0                                 1931.825000       460.430041           961  \n1                                  845.567568       745.377359           154  \n2                                   -1.000000       276.569767           905  \n3                                 1643.137255       741.363985           225  \n4                                 3946.180995      1241.236982          1071  \n\n[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "####### Load our Data\n",
    "#y - 'number_moved'\n",
    "#x - 'everything else that is or can be represented as a float.'\n",
    "devSet = pd.read_csv(\"./us_migration.csv\")\n",
    "devSet = devSet.loc[:, ~devSet.columns.str.contains('^Unnamed')]\n",
    "devSet = devSet.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "devSet = devSet.dropna(axis=1)\n",
    "devSet = devSet.drop(['sending'], axis = 1)\n",
    "\n",
    "print(devSet.head())\n",
    "\n",
    "y = torch.Tensor(devSet['US_MIG_05_10'].values)\n",
    "X = devSet.loc[:, devSet.columns != \"US_MIG_05_10\"].values\n",
    "\n",
    "mMScale = preprocessing.MinMaxScaler()\n",
    "X = mMScale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, split):\n",
    "\n",
    "    train_num = int(len(X) * split)\n",
    "    val_num = int(len(X) - train_num)\n",
    "\n",
    "    train_indices = random.sample(range(len(X)), train_num)\n",
    "    val_indices = [i for i in range(len(X)) if i not in train_indices]\n",
    "\n",
    "    x_train, x_val = X[train_indices], X[val_indices]\n",
    "    y_train, y_val = y[val_indices], y[val_indices]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Build and fit the Model\n",
    "lr = 1e-6\n",
    "batchSize = 50\n",
    "model = socialSig.SocialSigNet(X=X, outDim = batchSize)\n",
    "epochs = 1\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = train_test_split(X, y, .80)\n",
    "\n",
    "train = [(k,v) for k,v in zip(x_train, y_train)]\n",
    "val = [(k,v) for k,v in zip(x_val, y_val)]\n",
    "\n",
    "train = torch.utils.data.DataLoader(train, batch_size = batchSize, shuffle = True)\n",
    "val = torch.utils.data.DataLoader(val, batch_size = batchSize, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'010'"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "str(epoch) + str(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/heatherbaier/anaconda/envs/caoe/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/heatherbaier/anaconda/envs/caoe/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/heatherbaier/Desktop/CAOE/sig/socialSig.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(\"    W at beginning: \", torch.tensor(self.W))\n",
      "/Users/heatherbaier/Desktop/CAOE/sig/socialSig.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  taken = torch.take(batchX, construct_noOverlap_indices(torch.tensor(self.W, dtype = torch.float32), batchX.shape[0], self.W.shape[0]))\n",
      "/Users/heatherbaier/anaconda/envs/caoe/lib/python3.6/site-packages/torch/nn/functional.py:3455: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "    W at beginning:  tensor([2.1385e-02, 9.4206e-03, 9.6362e-02, 5.3210e-02, 1.4519e-05, 4.7531e-02,\n",
      "        8.8082e-02, 3.7379e-02, 8.0122e-02, 6.7977e-02, 6.6265e-03, 3.3945e-02,\n",
      "        6.7518e-02, 6.7380e-02, 5.9933e-02, 2.2598e-02, 7.0929e-02, 1.1315e-02,\n",
      "        7.4608e-02, 8.8387e-02, 4.0019e-02, 9.5174e-02, 6.5706e-02, 3.0055e-02,\n",
      "        5.6044e-02, 3.6465e-02, 7.0851e-02, 6.7270e-02, 8.3068e-02])\n",
      "    W at beginning:  tensor([2.1305e-02, 9.4286e-03, 9.6977e-02, 5.4738e-02, 1.7783e-05, 4.7232e-02,\n",
      "        8.8646e-02, 3.7940e-02, 8.0588e-02, 6.7504e-02, 6.7540e-03, 3.3867e-02,\n",
      "        6.7427e-02, 6.7903e-02, 6.0355e-02, 2.2637e-02, 6.9931e-02, 1.1264e-02,\n",
      "        7.4695e-02, 8.8060e-02, 4.0224e-02, 9.5238e-02, 6.5812e-02, 2.9787e-02,\n",
      "        5.6330e-02, 3.6731e-02, 7.1019e-02, 6.6993e-02, 8.2748e-02])\n",
      "    W at beginning:  tensor([0.0216, 0.0102, 0.0981, 0.0560, 0.0019, 0.0474, 0.0877, 0.0407, 0.0828,\n",
      "        0.0697, 0.0086, 0.0349, 0.0689, 0.0705, 0.0572, 0.0095, 0.0693, 0.0109,\n",
      "        0.0757, 0.0845, 0.0388, 0.0969, 0.0673, 0.0299, 0.0563, 0.0360, 0.0783,\n",
      "        0.0685, 0.0835])\n",
      "    W at beginning:  tensor([ 0.0133, -0.0274,  0.0351,  0.0858,  0.0067,  0.0536,  0.0031,  0.1435,\n",
      "         0.0078,  0.0901,  0.0261,  0.0372,  0.0571,  0.2707,  0.1183,  0.0093,\n",
      "         0.0814,  0.0175,  0.0060,  0.0457,  0.0129,  0.0966,  0.0487,  0.0291,\n",
      "         0.0336,  0.0381,  0.0632,  0.0694,  0.0691])\n",
      "    W at beginning:  tensor([ 0.0149, -0.0264,  0.0383,  0.0910,  0.0096,  0.0545,  0.0045,  0.1436,\n",
      "         0.0112,  0.0941,  0.0245,  0.0367,  0.0577,  0.2694,  0.1264,  0.0181,\n",
      "         0.0827,  0.0185,  0.0084,  0.0462,  0.0170,  0.1037,  0.0454,  0.0271,\n",
      "         0.0348,  0.0390,  0.0634,  0.0648,  0.0572])\n",
      "    W at beginning:  tensor([ 0.0119, -0.0284,  0.0336,  0.0882,  0.0080,  0.0553,  0.0029,  0.1432,\n",
      "         0.0123,  0.0991,  0.0362,  0.0397,  0.0587,  0.2684,  0.1371,  0.0147,\n",
      "         0.0793,  0.0211,  0.0075,  0.0447,  0.0214,  0.1233,  0.0558,  0.0280,\n",
      "         0.0344,  0.0353,  0.0625,  0.0673,  0.0526])\n",
      "    W at beginning:  tensor([ 1.1235e-02, -2.9630e-02,  3.1480e-02,  8.7979e-02,  7.9155e-03,\n",
      "         5.5984e-02, -9.6700e-05,  1.4613e-01,  1.1534e-02,  9.9155e-02,\n",
      "         3.3357e-02,  3.7526e-02,  5.8076e-02,  2.7287e-01,  1.3960e-01,\n",
      "         1.6952e-02,  7.8866e-02,  2.2443e-02,  9.0616e-03,  4.2746e-02,\n",
      "         2.0933e-02,  1.2510e-01,  5.5445e-02,  2.7335e-02,  3.5099e-02,\n",
      "         3.4919e-02,  6.2386e-02,  6.7636e-02,  5.3532e-02])\n",
      "    W at beginning:  tensor([ 0.0060, -0.0247,  0.0212,  0.0901,  0.0084,  0.0724, -0.0204,  0.1327,\n",
      "         0.0251,  0.0973,  0.0161,  0.0520,  0.0424,  0.3010,  0.1730,  0.0251,\n",
      "         0.0704,  0.0504,  0.0027,  0.0399,  0.0219,  0.1170,  0.0118,  0.0112,\n",
      "         0.0348,  0.0390,  0.0593,  0.0681,  0.0550])\n",
      "    W at beginning:  tensor([ 0.0088, -0.0266,  0.0206,  0.0899,  0.0085,  0.0724,  0.0033,  0.1300,\n",
      "         0.0220,  0.0961,  0.0160,  0.0519,  0.0488,  0.2994,  0.1710,  0.0211,\n",
      "         0.0696,  0.0502,  0.0022,  0.0384,  0.0152,  0.1134,  0.0114,  0.0110,\n",
      "         0.0347,  0.0392,  0.0578,  0.0651,  0.0549])\n",
      "/Users/heatherbaier/anaconda/envs/caoe/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/heatherbaier/anaconda/envs/caoe/lib/python3.6/site-packages/ipykernel_launcher.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "Epoch:  0\n",
      "  Train:\n",
      "    Loss:  9838980.6\n",
      "    MAE:  239.91559906005858\n",
      "  Val:\n",
      "    Loss:  9814683.45\n",
      "    MAE:  217.92244720458984\n"
     ]
    }
   ],
   "source": [
    "best_mae = 9000000000000000000\n",
    "best_model_wts = deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\n",
    "    for phase in ['train','val']:\n",
    "\n",
    "\n",
    "\n",
    "        if phase == 'train':\n",
    "\n",
    "            c = 1\n",
    "            running_train_mae, running_train_loss = 0, 0\n",
    "\n",
    "            # print(\"In training\")\n",
    "\n",
    "            for inputs, output in train:\n",
    "\n",
    "                if len(inputs) == batchSize:\n",
    "\n",
    "                    # print(c)\n",
    "                    c += 1\n",
    "\n",
    "                    inputs = torch.tensor(inputs, dtype = torch.float32, requires_grad = True)\n",
    "                    output = torch.reshape(torch.tensor(output, dtype = torch.float32, requires_grad = True), (batchSize,1))\n",
    "\n",
    "                    # Forward pass\n",
    "                    y_pred = model(inputs, str(epoch) + str(c))\n",
    "                    loss = criterion(y_pred, output)  \n",
    "                    \n",
    "                    # Zero gradients, perform a backward pass, and update the weights.\n",
    "                    optimizer.zero_grad()\n",
    "                    grad = torch.autograd.grad(outputs = loss, inputs = inputs, retain_graph = True)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Update the coordinate weights\n",
    "                    # https://discuss.pytorch.org/t/updatation-of-parameters-without-using-optimizer-step/34244/4\n",
    "                    with torch.no_grad():\n",
    "                        for name, p in model.named_parameters():\n",
    "                            if name == 'SocialSig.W':\n",
    "                                new_val = update_function(p, grad[0], loss, lr)\n",
    "                                p.copy_(new_val)\n",
    "\n",
    "                    running_train_mae += mae(y_pred, output).item()\n",
    "                    running_train_loss += loss.item()\n",
    "\n",
    "        if phase == 'val':\n",
    "\n",
    "            c = 1\n",
    "            running_val_mae, running_val_loss,  = 0, 0\n",
    "\n",
    "            # print(\"In validation\")\n",
    "\n",
    "            for inputs, output in val:\n",
    "\n",
    "                if len(inputs) == batchSize:\n",
    "\n",
    "                    # print(c)\n",
    "                    c += 1\n",
    "\n",
    "                    inputs = torch.tensor(inputs, dtype = torch.float32, requires_grad = True)\n",
    "                    output = torch.reshape(torch.tensor(output, dtype = torch.float32, requires_grad = True), (batchSize,1))\n",
    "\n",
    "                    # Forward pass\n",
    "                    y_pred = model(inputs, 1)\n",
    "                    loss = criterion(y_pred, output)  \n",
    "\n",
    "                    running_val_mae += mae(y_pred, output).item()\n",
    "                    running_val_loss += loss.item()\n",
    "                    \n",
    "                    if mae(y_pred, output).item() < best_mae:\n",
    "                        best_mae = mae(y_pred, output).item()\n",
    "                        best_model_wts = deepcopy(model.state_dict())\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "    print(\"Epoch: \", epoch)  \n",
    "    print(\"  Train:\")\n",
    "    print(\"    Loss: \", running_train_loss / c)      \n",
    "    print(\"    MAE: \", running_train_mae / c)\n",
    "    print(\"  Val:\")\n",
    "    print(\"    Loss: \", running_val_loss / c)      \n",
    "    print(\"    MAE: \", running_val_mae / c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.15086376, 0.01904088, 0.17205208, ..., 0.15731832, 0.13144279,\n",
       "        0.09660652],\n",
       "       [0.12192231, 0.04674133, 0.18605187, ..., 0.1205067 , 0.05757128,\n",
       "        0.15626403],\n",
       "       [0.08485822, 0.04340877, 0.10657675, ..., 0.09579383, 0.        ,\n",
       "        0.05811292],\n",
       "       ...,\n",
       "       [0.2560983 , 0.06107871, 0.22930659, ..., 0.24920383, 0.04052688,\n",
       "        0.29325165],\n",
       "       [0.30418829, 0.12659953, 0.30113786, ..., 0.29673268, 0.09956838,\n",
       "        0.2626708 ],\n",
       "       [0.45947368, 0.26082901, 0.4582634 , ..., 0.44939452, 0.22173665,\n",
       "        0.33150409]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "tensor([[169.2038]], grad_fn=<AddmmBackward>)\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "tensor([[156.2059]], grad_fn=<AddmmBackward>)\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "tensor([[163.5242]], grad_fn=<AddmmBackward>)\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "tensor([[166.0314]], grad_fn=<AddmmBackward>)\n",
      "    W at beginning:  tensor([ 0.0114, -0.0293,  0.0205,  0.0906,  0.0087,  0.0729,  0.0042,  0.1181,\n",
      "         0.0211,  0.0962,  0.0164,  0.0528,  0.0474,  0.2964,  0.1709,  0.0216,\n",
      "         0.0699,  0.0503,  0.0016,  0.0387,  0.0154,  0.1112,  0.0114,  0.0112,\n",
      "         0.0349,  0.0400,  0.0589,  0.0646,  0.0560])\n",
      "tensor([[177.9655]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "\n",
    "    imput = torch.reshape(torch.tensor(X[i], dtype = torch.float32), (1,1,29))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    print(model(imput, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[2.7388e-01, 3.0840e-02, 2.5688e-01, 2.7936e-01, 3.6218e-02, 1.8417e-01,\n",
       "         1.2314e-01, 0.0000e+00, 0.0000e+00, 2.4442e-01, 2.6865e-01, 2.7435e-01,\n",
       "         0.0000e+00, 3.8383e-02, 2.8620e-01, 1.0488e-01, 2.7305e-01, 2.4308e-01,\n",
       "         1.2009e-02, 4.6390e-03, 2.4740e-01, 1.1830e-01, 1.0000e+00, 1.2805e-01,\n",
       "         2.7101e-01, 1.9818e-01, 2.6964e-01, 1.1546e-01, 2.7110e-01],\n",
       "        [3.8522e-02, 1.3040e-02, 4.8051e-02, 3.5860e-02, 6.5743e-03, 2.6583e-02,\n",
       "         5.7025e-02, 0.0000e+00, 5.1311e-02, 4.7917e-02, 5.6560e-02, 4.9159e-02,\n",
       "         0.0000e+00, 4.6879e-03, 1.3266e-01, 1.8648e-02, 6.1607e-02, 2.2079e-02,\n",
       "         1.7460e-03, 2.3633e-03, 1.0237e-01, 5.4286e-03, 1.0000e+00, 2.7162e-02,\n",
       "         3.8032e-02, 2.5502e-02, 3.7844e-02, 1.7950e-02, 4.9828e-02],\n",
       "        [3.4198e-02, 0.0000e+00, 3.1788e-02, 3.2782e-02, 2.3091e-03, 2.3490e-02,\n",
       "         0.0000e+00, 3.3332e-05, 9.1619e-05, 3.4928e-02, 8.4081e-02, 4.4449e-02,\n",
       "         0.0000e+00, 4.3339e-03, 1.8147e-01, 2.0360e-02, 3.8448e-02, 2.2578e-02,\n",
       "         8.4638e-04, 1.5609e-03, 2.5642e-01, 5.0979e-03, 1.0000e+00, 1.9591e-02,\n",
       "         3.3844e-02, 3.8449e-02, 3.3383e-02, 5.0139e-03, 7.6865e-03],\n",
       "        [2.3281e-02, 0.0000e+00, 3.9995e-02, 2.1494e-02, 0.0000e+00, 1.7799e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3106e-02, 1.3961e-02, 2.9961e-02,\n",
       "         0.0000e+00, 4.3782e-03, 2.3512e-01, 8.6578e-03, 9.5833e-02, 2.1071e-02,\n",
       "         2.6487e-04, 3.7922e-04, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "         2.3034e-02, 0.0000e+00, 2.2883e-02, 0.0000e+00, 1.2998e-03],\n",
       "        [2.0921e-01, 4.4923e-02, 2.1290e-01, 2.0195e-01, 1.9600e-02, 1.3742e-01,\n",
       "         1.1288e-01, 3.7611e-02, 5.5524e-02, 1.9996e-01, 2.0788e-01, 2.4176e-01,\n",
       "         0.0000e+00, 3.9808e-02, 2.4944e-01, 7.7954e-02, 1.9615e-01, 1.4628e-01,\n",
       "         1.2730e-02, 7.3144e-03, 2.1051e-01, 6.9689e-02, 1.0000e+00, 9.3800e-02,\n",
       "         2.0665e-01, 1.6759e-01, 2.0395e-01, 1.0343e-01, 1.9592e-01],\n",
       "        [2.4739e-01, 2.0309e-01, 2.5168e-01, 2.4516e-01, 4.0134e-02, 1.6648e-01,\n",
       "         1.0699e-01, 7.5307e-02, 5.8991e-02, 2.4289e-01, 2.4459e-01, 2.6559e-01,\n",
       "         0.0000e+00, 3.3407e-02, 2.6680e-01, 1.2044e-01, 2.2077e-01, 1.9168e-01,\n",
       "         1.1707e-02, 4.6583e-03, 6.6981e-02, 6.3210e-02, 1.0000e+00, 1.1580e-01,\n",
       "         2.4463e-01, 1.5016e-01, 2.5228e-01, 1.3563e-01, 1.0917e-01],\n",
       "        [2.9384e-02, 0.0000e+00, 2.6786e-02, 2.7921e-02, 4.0031e-03, 2.0296e-02,\n",
       "         6.4615e-02, 1.1460e-02, 5.5063e-02, 3.1986e-02, 1.9470e-02, 3.6692e-02,\n",
       "         0.0000e+00, 5.5223e-03, 1.0022e-01, 1.0607e-02, 2.0487e-02, 2.3229e-02,\n",
       "         1.1723e-03, 1.3652e-03, 1.9359e-02, 4.1163e-02, 1.0000e+00, 0.0000e+00,\n",
       "         2.9053e-02, 4.1366e-02, 2.8200e-02, 1.8936e-02, 3.0097e-02],\n",
       "        [1.6058e-01, 9.0202e-02, 1.8378e-01, 1.4656e-01, 4.1085e-02, 1.4942e-01,\n",
       "         1.0090e-01, 8.2279e-02, 0.0000e+00, 1.4465e-01, 1.5312e-01, 1.7268e-01,\n",
       "         1.3833e-03, 2.1833e-02, 1.9427e-01, 7.2510e-02, 1.7541e-01, 1.3406e-01,\n",
       "         8.2984e-03, 3.8347e-03, 3.9307e-01, 7.0554e-02, 1.0000e+00, 8.6764e-02,\n",
       "         1.5788e-01, 1.0030e-01, 1.6472e-01, 4.2191e-02, 1.0984e-01],\n",
       "        [2.0365e-01, 7.3052e-02, 2.4116e-01, 1.8855e-01, 2.5637e-02, 1.2851e-01,\n",
       "         1.1005e-01, 3.9958e-02, 5.6121e-02, 1.9319e-01, 2.0747e-01, 2.1467e-01,\n",
       "         1.1736e-02, 3.1751e-02, 2.3047e-01, 8.2616e-02, 2.6615e-01, 1.5826e-01,\n",
       "         5.1630e-02, 2.3659e-02, 1.1326e-01, 6.0390e-02, 1.0000e+00, 9.3823e-02,\n",
       "         2.0145e-01, 1.6961e-01, 1.9888e-01, 8.0093e-02, 1.5139e-01],\n",
       "        [1.6970e-01, 7.4904e-02, 1.8935e-01, 1.5941e-01, 1.8124e-02, 1.2285e-01,\n",
       "         9.8598e-02, 0.0000e+00, 4.7712e-02, 1.6891e-01, 1.6594e-01, 1.9633e-01,\n",
       "         4.3706e-03, 2.5716e-02, 2.2867e-01, 8.4214e-02, 1.2098e-01, 1.2501e-01,\n",
       "         1.1106e-02, 7.0428e-03, 0.0000e+00, 4.4021e-02, 1.0000e+00, 9.3105e-02,\n",
       "         1.6759e-01, 8.1891e-02, 1.7020e-01, 1.0937e-01, 1.4608e-01],\n",
       "        [8.4331e-02, 6.3937e-03, 9.7737e-02, 7.9306e-02, 7.2285e-03, 5.8668e-02,\n",
       "         8.1548e-02, 2.1618e-02, 7.4491e-02, 7.5026e-02, 7.7640e-02, 1.0716e-01,\n",
       "         0.0000e+00, 1.4198e-02, 1.8456e-01, 2.6801e-02, 9.0520e-02, 6.3966e-02,\n",
       "         5.1138e-03, 5.5468e-03, 5.8350e-02, 1.5249e-02, 1.0000e+00, 5.8857e-02,\n",
       "         8.3167e-02, 3.3850e-02, 8.4162e-02, 5.0929e-02, 9.1505e-02],\n",
       "        [2.5301e-01, 7.0494e-02, 2.4861e-01, 2.5261e-01, 2.5164e-02, 1.8115e-01,\n",
       "         1.1999e-01, 5.6296e-02, 1.3461e-01, 2.4151e-01, 2.4419e-01, 2.8332e-01,\n",
       "         1.3065e-01, 4.7259e-02, 2.9718e-01, 8.8987e-02, 2.7954e-01, 1.7463e-01,\n",
       "         2.5605e-02, 2.9278e-02, 1.3861e-01, 1.3184e-01, 1.0000e+00, 1.1439e-01,\n",
       "         2.5063e-01, 1.9321e-01, 2.4684e-01, 1.6099e-01, 2.2813e-01],\n",
       "        [1.5356e-01, 2.6743e-02, 1.6355e-01, 1.4653e-01, 1.5101e-02, 1.0342e-01,\n",
       "         1.0601e-01, 2.3844e-02, 0.0000e+00, 1.5218e-01, 1.5441e-01, 1.6613e-01,\n",
       "         7.0865e-04, 1.7603e-02, 2.3436e-01, 6.1595e-02, 1.7753e-01, 1.2726e-01,\n",
       "         9.6233e-03, 6.8085e-03, 3.5004e-02, 3.9319e-02, 1.0000e+00, 8.9073e-02,\n",
       "         1.5138e-01, 7.3516e-02, 1.6386e-01, 4.2372e-02, 7.8316e-02],\n",
       "        [2.0061e-01, 5.8901e-02, 2.0148e-01, 1.9533e-01, 2.0819e-02, 1.2150e-01,\n",
       "         9.1889e-02, 3.7007e-02, 1.7540e-02, 1.9748e-01, 2.1577e-01, 2.2827e-01,\n",
       "         0.0000e+00, 3.5231e-02, 2.4563e-01, 1.1641e-01, 2.0024e-01, 1.3808e-01,\n",
       "         1.5355e-02, 1.8239e-02, 4.5649e-02, 5.9231e-02, 1.0000e+00, 9.6643e-02,\n",
       "         1.9814e-01, 5.3808e-01, 1.7435e-01, 7.5263e-02, 1.4868e-01],\n",
       "        [9.2427e-02, 3.1187e-02, 8.5081e-02, 8.8253e-02, 2.5471e-02, 6.4657e-02,\n",
       "         8.7187e-02, 3.3332e-05, 0.0000e+00, 9.7802e-02, 6.6808e-02, 1.1386e-01,\n",
       "         0.0000e+00, 7.2185e-03, 1.9594e-01, 4.1522e-02, 1.2835e-01, 7.3791e-02,\n",
       "         2.8372e-03, 1.5616e-03, 5.2440e-02, 3.9740e-02, 1.0000e+00, 5.2138e-02,\n",
       "         9.1396e-02, 9.5489e-02, 8.9893e-02, 5.8003e-02, 3.3618e-02]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[  8,  12,   7,  19,  18,  13,   4,   1,  28,  21,  15,  23,  20,  27,\n           5,  10,  17,   2,   6,   3,  26,  24,   0,  25,   9,  11,  16,  14,\n          22],\n        [ 37,  41,  36,  48,  47,  42,  33,  30,  57,  50,  44,  52,  49,  56,\n          34,  39,  46,  31,  35,  32,  55,  53,  29,  54,  38,  40,  45,  43,\n          51],\n        [ 66,  70,  65,  77,  76,  71,  62,  59,  86,  79,  73,  81,  78,  85,\n          63,  68,  75,  60,  64,  61,  84,  82,  58,  83,  67,  69,  74,  72,\n          80],\n        [ 95,  99,  94, 106, 105, 100,  91,  88, 115, 108, 102, 110, 107, 114,\n          92,  97, 104,  89,  93,  90, 113, 111,  87, 112,  96,  98, 103, 101,\n         109],\n        [124, 128, 123, 135, 134, 129, 120, 117, 144, 137, 131, 139, 136, 143,\n         121, 126, 133, 118, 122, 119, 142, 140, 116, 141, 125, 127, 132, 130,\n         138],\n        [153, 157, 152, 164, 163, 158, 149, 146, 173, 166, 160, 168, 165, 172,\n         150, 155, 162, 147, 151, 148, 171, 169, 145, 170, 154, 156, 161, 159,\n         167],\n        [182, 186, 181, 193, 192, 187, 178, 175, 202, 195, 189, 197, 194, 201,\n         179, 184, 191, 176, 180, 177, 200, 198, 174, 199, 183, 185, 190, 188,\n         196],\n        [211, 215, 210, 222, 221, 216, 207, 204, 231, 224, 218, 226, 223, 230,\n         208, 213, 220, 205, 209, 206, 229, 227, 203, 228, 212, 214, 219, 217,\n         225],\n        [240, 244, 239, 251, 250, 245, 236, 233, 260, 253, 247, 255, 252, 259,\n         237, 242, 249, 234, 238, 235, 258, 256, 232, 257, 241, 243, 248, 246,\n         254],\n        [269, 273, 268, 280, 279, 274, 265, 262, 289, 282, 276, 284, 281, 288,\n         266, 271, 278, 263, 267, 264, 287, 285, 261, 286, 270, 272, 277, 275,\n         283]])\n"
     ]
    }
   ],
   "source": [
    "test = [9.2427e-02, 3.1187e-02, 8.5081e-02, 8.8253e-02, 2.5471e-02, 6.4657e-02,\n",
    "         8.7187e-02, 3.3332e-05, 0.0000e+00, 9.7802e-02, 6.6808e-02, 1.1386e-01,\n",
    "         0.0000e+00, 7.2185e-03, 1.9594e-01, 4.1522e-02, 1.2835e-01, 7.3791e-02,\n",
    "         2.8372e-03, 1.5616e-03, 5.2440e-02, 3.9740e-02, 1.0000e+00, 5.2138e-02,\n",
    "         9.1396e-02, 9.5489e-02, 8.9893e-02, 5.8003e-02, 3.3618e-02]\n",
    "\n",
    "\n",
    "\n",
    "print(construct_noOverlap_indices(torch.tensor(test, dtype = torch.float32), 10, 29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"./trained_weights.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 50,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            }, \"./trained_weights_nosending5.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-012362d859f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Prep the batch for a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mbatchObs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m#batchObs = [i for i in range(0, batchSize)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmodelX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchObs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/caoe/lib/python3.6/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    for k in range(math.ceil(len(y)/batchSize)):\n",
    "        # Prep the batch for a forward pass\n",
    "        batchObs = random.sample(range(0, len(y)), batchSize)\n",
    "        #batchObs = [i for i in range(0, batchSize)]\n",
    "        modelX = X[batchObs]\n",
    "        modelX = torch.tensor(list(modelX), requires_grad = True, dtype = torch.float32)\n",
    "        modely = torch.reshape(torch.tensor(y[batchObs], dtype = torch.float32), (batchSize,1))\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(modelX, t)\n",
    "        loss = criterion(y_pred, modely)  \n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        grad = torch.autograd.grad(outputs=loss, inputs=modelX, retain_graph = True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the coordinate weights\n",
    "        # https://discuss.pytorch.org/t/updatation-of-parameters-without-using-optimizer-step/34244/4\n",
    "        with torch.no_grad():\n",
    "            for name, p in model.named_parameters():\n",
    "                if name == 'SocialSig.W':\n",
    "                    new_val = update_function(p, grad[0], loss, lr)\n",
    "                    p.copy_(new_val)\n",
    "        \n",
    "\n",
    "        print(\"Epoch: \" + str(t) + \" Batch: \" + str(k))\n",
    "        print(\"    Loss:     \", loss.item(), \"     MAE: \", mae(y_pred, modely).item())\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if loss.item() < 0:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}