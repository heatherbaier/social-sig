{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class makeCoords(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Layer to make the coordiante pairs\n",
    "        The self.coords parameter is what we care about training in the Net so we initialize it here\n",
    "        and then we'll continue passing the self.coords Param (as X) through the Net\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.coords = torch.nn.Parameter(torch.tensor([1,2,3,4,5,6,7,8], dtype = torch.float32))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"    New Coordinates: \", [i.item() for i in self.coords])\n",
    "        # We have to do ~something~ to the Param here, I think of it kind of like 'activating' the parameter\n",
    "        # We can also add or subtract to 'activate' but this just returns the self.coords without any changes\n",
    "        return 1 * self.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialSig(torch.nn.Module):\n",
    "    '''\n",
    "    Class to create the social signature image\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Basic steps for class:\n",
    "            1. Randomly initalize 'weights' which I believe are actually the coords of the points\n",
    "            2. Train kriggin model to interpolate points in between\n",
    "            3. Predict what the points would be within a 224x224 matrix and output the resulting matrix\n",
    "        '''\n",
    "        super(SocialSig, self).__init__()\n",
    "        self.outDim = [10,10]\n",
    "        self.padding = torch.nn.ConstantPad1d((0,92), 1)\n",
    "\n",
    "    def forward(self, inputs, x):\n",
    "        \n",
    "        self.X = inputs\n",
    "        \n",
    "        # Create our blank grid that we will fill with the IDW values\n",
    "        self.grid = self.__make_blank_coord_grid()\n",
    "        \n",
    "        # I did this moslty because it was easier to create a var called 'coords' then to replace \n",
    "        # every instance of it with x. Using clone preserves the trainable parameters, so that we \n",
    "        # can eventually return coords instead of x from this layer\n",
    "        coords = x.clone()\n",
    "        \n",
    "        # Get the 'image' from IDW\n",
    "        tensorRet = self.IDW(coords, inputs)\n",
    "        \n",
    "        # Flatten the IDW image into tensor list\n",
    "        tensorRet = tensorRet.flatten()\n",
    "        \n",
    "        # Pad the image with 1's (this is to get it into the final shape of the iamge.)\n",
    "        # At this point we are done with the actual coord values but we still need the 'shell' of the \n",
    "        # parameter so we are reshaping it into what we need for the final output\n",
    "        coords = self.padding(coords)\n",
    "        \n",
    "        \n",
    "        # I need to fix something here but we have two matrices: \n",
    "        #    1) 'coords' is the original coords matrix that we just padded with a bunch of 1's \n",
    "        #    2) 'tensorRet' which has all of the IDW values\n",
    "        # They're the same shape so we just multiply in order to 'replace' the value in our coords tensor\n",
    "        # with those from our tensorRet. The output from this is our image that we then keep feeding through \n",
    "        # the remaining layers in the forward function within the Net\n",
    "        # TO FIX: The first 8 values of coords are still the parameterized coords so you need to zero them out first\n",
    "        return coords * tensorRet\n",
    "       \n",
    "    \n",
    "    def IDW(self, coords, inputs):\n",
    "        '''\n",
    "        Train the IDW model to predict all of the points that are between known points\n",
    "        '''\n",
    "        coords = torch.clamp(coords, min=0, max=self.outDim[1])\n",
    "                \n",
    "        for cell in range(0, len(self.grid)):\n",
    "            weightedVals = []\n",
    "            for column in range(0, len(inputs)):\n",
    "                xCoordLookup = column * 2\n",
    "                yCoordLookup = xCoordLookup + 1\n",
    "            \n",
    "                measurementCellValue = self.X[column]\n",
    "\n",
    "                estCellX = self.grid[cell][0]\n",
    "                estCellY = self.grid[cell][1]\n",
    "\n",
    "                measureCellX = coords[xCoordLookup]\n",
    "                measureCellY = coords[yCoordLookup]\n",
    "\n",
    "                A2 = abs(estCellX - measureCellX)**2\n",
    "                B2 = abs(estCellY - measureCellY)**2\n",
    "                C2 = math.sqrt(A2+B2) \n",
    "                if(C2 == 0):\n",
    "                    C2 = 1\n",
    "                \n",
    "                measurementCellValue = [measurementCellValue.numpy().tolist()]\n",
    "                weightedVals.append(measurementCellValue[0] * (1/(C2**2)))\n",
    "            self.grid[cell] = sum(weightedVals)\n",
    "        numpyGrid = torch.from_numpy(np.reshape(np.array(self.grid), (1,1,self.outDim[0],self.outDim[1])))\n",
    "        tensorGrid = torch.tensor(numpyGrid, dtype=torch.float)\n",
    "\n",
    "        return tensorGrid \n",
    "        \n",
    "    def __make_blank_coord_grid(self):\n",
    "        '''\n",
    "        Make a blank coordinate grid to fill in with real data later\n",
    "        '''\n",
    "        outDim = [10, 10]\n",
    "        return [[x,y] for x in range(0, outDim[0]) for y in range(0,outDim[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialSigNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.makeCoords = makeCoords()               # Layer to make the coordinate pairs\n",
    "        self.linear2 = torch.nn.Linear(10, 1)        # Linear Layer\n",
    "        self.SocialSig = SocialSig()                 # SocialSig layer that creates the nxn image\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, 10, 1)   # 1-D convolutional layer\n",
    "\n",
    "\n",
    "    def forward(self, x, inputs):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        - We have to keep x as the original training param the whole time, so only\n",
    "        perform transformations on the variable x, which begins as a set of randomly \n",
    "        initalized points, then gets transformed into an image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Here we make our coordinate list\n",
    "        x = self.makeCoords(x)\n",
    "        x = self.SocialSig(inputs, x)\n",
    "        \n",
    "        # Here we reshape it into an imagy shape (not sure if this is totally neccessary)\n",
    "        x = torch.reshape(x, (1, 1, 10, 10))\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = torch.flatten(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random tensors to hold input and outputs.\n",
    "x = torch.tensor([-1.6848])\n",
    "y = torch.tensor([0.0359, 0.7196, 0.7006, 7.2164, 5.4166, 0.4062, 2.2501, 6.6463])\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = SocialSigNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "    New Coordinates:  [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "    Predicted Y:  tensor([-0.4477], grad_fn=<AddBackward0>)\n",
      "    Loss:  tensor(154.3757, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Epoch:  1\n",
      "    New Coordinates:  [0.9999999403953552, 1.999999761581421, 2.9999959468841553, 3.9999990463256836, 4.999999523162842, 6.0, 7.0, 8.0]\n",
      "    Predicted Y:  tensor([1.5086e+12], grad_fn=<AddBackward0>)\n",
      "    Loss:  tensor(1.8208e+25, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Epoch:  2\n",
      "    New Coordinates:  [16829.71484375, 123585.65625, 1782109.0, 455306.46875, 140058.34375, 22846.23046875, 4027.679931640625, 29038.208984375]\n",
      "    Predicted Y:  tensor([2.8492e+38], grad_fn=<AddBackward0>)\n",
      "    Loss:  tensor(inf, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Epoch:  3\n",
      "    New Coordinates:  [nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "    Predicted Y:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "    Loss:  tensor(nan, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "    New Coordinates:  [nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "    Predicted Y:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "    Loss:  tensor(nan, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heatherbaier/anaconda/envs/caoe/lib/python3.6/site-packages/ipykernel_launcher.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "for t in range(5):\n",
    "    \n",
    "    print(\"Epoch: \", t)\n",
    "    \n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    # I pass the inputs twice -- This might be fixable later (after sleep) but the idea was that the first\n",
    "    # x param gets immediately reset to the coordinate 'weights' and the second is preserved until we need it \n",
    "    # for the IDW\n",
    "    # ^^ I think I know a better way to do that but for now...\n",
    "    y_pred = model(x, x)\n",
    "    \n",
    "    print(\"    Predicted Y: \", y_pred)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    # This autograd stuff is seeming to me vaguely like recursion. It seems like the parameters that will\n",
    "    # be updated in loss.backward() are those that are directly returned by the forward pass (i.e. the \n",
    "    # params linked to y_pred)\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(\"    Loss: \", loss)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caoe",
   "language": "python",
   "name": "caoe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
