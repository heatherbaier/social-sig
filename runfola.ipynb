{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import openturns as ot\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-523-c56267acb2aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutDim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSocialSig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "X = [14, 12, 13, 12]\n",
    "outDim = [10,10]\n",
    "\n",
    "test = SocialSig(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialSig():\n",
    "    '''\n",
    "    Class to create the social signature image\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Basic steps for class:\n",
    "            1. Randomly initalize 'weights' which I believe are actually the coords of the points\n",
    "            2. Train kriggin model to interpolate points in between\n",
    "            3. Predict what the points would be within a 224x224 matrix and output the resulting matrix\n",
    "        '''\n",
    "        super(SocialSig, self).__init__()\n",
    "        self.outDim = [10,10]\n",
    "        self.coords = torch.nn.Parameter(torch.tensor(random.choices(range(0,outDim[1]), k=16), dtype=torch.float32)) \n",
    "\n",
    "    def forward(self, input):\n",
    "        self.X = input\n",
    "        self.grid = self.__make_blank_coord_grid()\n",
    "        tensorRet = self.IDW()\n",
    "        return tensorRet\n",
    "       \n",
    "    \n",
    "    def IDW(self):\n",
    "        '''\n",
    "        Train the IDW model to predict all of the points that are between known points\n",
    "        '''\n",
    "        self.coords = torch.clamp(self.coords, min=0, max=self.outDim[1])\n",
    "        \n",
    "        for cell in range(0, len(self.grid)):\n",
    "            weightedVals = []\n",
    "            for column in range(0, len(X)):\n",
    "                xCoordLookup = column * 2\n",
    "                yCoordLookup = xCoordLookup + 1\n",
    "            \n",
    "\n",
    "                measurementCellValue = self.X[column]\n",
    "\n",
    "                estCellX = self.grid[cell][0]\n",
    "                estCellY = self.grid[cell][1]\n",
    "\n",
    "                measureCellX = self.coords[xCoordLookup]\n",
    "                measureCellY = self.coords[yCoordLookup]\n",
    "\n",
    "                A2 = abs(estCellX - measureCellX)**2\n",
    "                B2 = abs(estCellY - measureCellY)**2\n",
    "                C2 = math.sqrt(A2+B2) \n",
    "                if(C2 == 0):\n",
    "                    C2 = 1\n",
    "                \n",
    "                weightedVals.append(measurementCellValue[0] * (1/(C2**2)))\n",
    "            self.grid[cell] = sum(weightedVals)\n",
    "        numpyGrid = torch.from_numpy(np.reshape(np.array(self.grid), (1,1,self.outDim[0],self.outDim[1])))\n",
    "        tensorGrid = torch.tensor(numpyGrid, dtype=torch.float)\n",
    "        print(self.coords)\n",
    "\n",
    "        return tensorGrid \n",
    "        \n",
    "    def __make_blank_coord_grid(self):\n",
    "        '''\n",
    "        Make a blank coordinate grid to fill in with real data later\n",
    "        '''\n",
    "        return [[x,y] for x in range(0, outDim[0]) for y in range(0,outDim[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[223], [146], [68], [196]]"
      ]
     },
     "metadata": {},
     "execution_count": 641
    }
   ],
   "source": [
    "# THIS IS WHAT YOU WILL USE TO CREATE YOUR INPUT LAYER--THIS SHOULD BE A LIST WITH 81 ELEMENTS (THE ROW IN THE DF)\n",
    "def make_example_data(seed, size):\n",
    "    '''\n",
    "    Create the training dataset by randomly assigning coordiantes to the known data (i.e the data within a row)\n",
    "    '''\n",
    "    points = [[i] for i in list(np.random.randint(low = 0, high = 255, size = size).flatten())]\n",
    "    return points\n",
    "\n",
    "\n",
    "make_example_data(14, size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.SocialSig = SocialSig()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 10, 1)\n",
    "        self.linear1 = torch.nn.Linear(10, 1)\n",
    "        self.linear2 = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        print(input)\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        sig = self.SocialSig.forward(input)\n",
    "        print(sig)\n",
    "        sig = self.conv1(sig)\n",
    "        print(sig.size()) #1,10,1,1\n",
    "        sig = torch.flatten(sig, 1)\n",
    "        print(sig.size())#1,10\n",
    "        sig = self.linear1(sig)\n",
    "\n",
    "        return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above\n",
    "x = make_example_data(14, size=4)\n",
    "model = TwoLayerNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[195], [226], [87], [183]]\ntensor([2., 8., 9., 5., 6., 9., 0., 1., 8., 0., 4., 3., 0., 0., 4., 6.],\n       grad_fn=<ClampBackward>)\ntensor([[[[188.7433, 189.8791, 191.4096,  56.3413,  34.2657,  30.9007,  36.3844,\n            48.9172,  57.3472,  46.6059],\n          [ 97.8601, 190.7025, 101.0418,  48.8498,  34.9875,  35.9179,  52.0742,\n           108.7695, 205.1021, 106.6204],\n          [ 43.5978,  54.2940,  47.2517,  36.6122,  32.9064,  38.1477,  63.0603,\n           208.1891, 207.4670, 206.6056],\n          [ 25.9716,  29.7713,  30.0925,  29.1603,  30.3042,  36.5778,  55.3238,\n           113.9090, 211.8774, 114.0197],\n          [ 19.1759,  21.9084,  23.9283,  25.8422,  28.7623,  34.1087,  44.2230,\n            61.1873,  75.6124,  68.5497],\n          [ 16.2829,  19.0830,  22.1518,  25.6970,  29.8226,  34.5394,  40.6541,\n            51.2000,  76.6796, 115.6187],\n          [ 15.1046,  18.4827,  23.0270,  29.1324,  36.2404,  41.8678,  45.0167,\n            53.1469, 113.8960, 109.3406],\n          [ 14.7051,  19.0083,  25.9813,  37.9542,  56.4574,  70.1683,  63.0971,\n            55.3029,  70.5520, 107.4195],\n          [ 14.4812,  19.7270,  29.7652,  53.2629, 122.2569, 236.9708, 126.6235,\n            63.1753,  47.0361,  41.7441],\n          [ 13.9640,  19.5658,  31.1369,  63.2214, 233.5922, 234.7287, 236.2390,\n            68.6564,  39.1984,  28.9537]]]])\ntorch.Size([1, 10, 1, 1])\ntorch.Size([1, 10])\nPRED:  tensor([[-6.2799]], grad_fn=<AddmmBackward>)\n\n\n[[195], [226], [87], [183]]\ntensor([2., 8., 9., 5., 6., 9., 0., 1., 8., 0., 4., 3., 0., 0., 4., 6.],\n       grad_fn=<ClampBackward>)\ntensor([[[[188.7433, 189.8791, 191.4096,  56.3413,  34.2657,  30.9007,  36.3844,\n            48.9172,  57.3472,  46.6059],\n          [ 97.8601, 190.7025, 101.0418,  48.8498,  34.9875,  35.9179,  52.0742,\n           108.7695, 205.1021, 106.6204],\n          [ 43.5978,  54.2940,  47.2517,  36.6122,  32.9064,  38.1477,  63.0603,\n           208.1891, 207.4670, 206.6056],\n          [ 25.9716,  29.7713,  30.0925,  29.1603,  30.3042,  36.5778,  55.3238,\n           113.9090, 211.8774, 114.0197],\n          [ 19.1759,  21.9084,  23.9283,  25.8422,  28.7623,  34.1087,  44.2230,\n            61.1873,  75.6124,  68.5497],\n          [ 16.2829,  19.0830,  22.1518,  25.6970,  29.8226,  34.5394,  40.6541,\n            51.2000,  76.6796, 115.6187],\n          [ 15.1046,  18.4827,  23.0270,  29.1324,  36.2404,  41.8678,  45.0167,\n            53.1469, 113.8960, 109.3406],\n          [ 14.7051,  19.0083,  25.9813,  37.9542,  56.4574,  70.1683,  63.0971,\n            55.3029,  70.5520, 107.4195],\n          [ 14.4812,  19.7270,  29.7652,  53.2629, 122.2569, 236.9708, 126.6235,\n            63.1753,  47.0361,  41.7441],\n          [ 13.9640,  19.5658,  31.1369,  63.2214, 233.5922, 234.7287, 236.2390,\n            68.6564,  39.1984,  28.9537]]]])\ntorch.Size([1, 10, 1, 1])\ntorch.Size([1, 10])\nPRED:  tensor([[539.3969]], grad_fn=<AddmmBackward>)\n\n\n[[195], [226], [87], [183]]\ntensor([2., 8., 9., 5., 6., 9., 0., 1., 8., 0., 4., 3., 0., 0., 4., 6.],\n       grad_fn=<ClampBackward>)\ntensor([[[[188.7433, 189.8791, 191.4096,  56.3413,  34.2657,  30.9007,  36.3844,\n            48.9172,  57.3472,  46.6059],\n          [ 97.8601, 190.7025, 101.0418,  48.8498,  34.9875,  35.9179,  52.0742,\n           108.7695, 205.1021, 106.6204],\n          [ 43.5978,  54.2940,  47.2517,  36.6122,  32.9064,  38.1477,  63.0603,\n           208.1891, 207.4670, 206.6056],\n          [ 25.9716,  29.7713,  30.0925,  29.1603,  30.3042,  36.5778,  55.3238,\n           113.9090, 211.8774, 114.0197],\n          [ 19.1759,  21.9084,  23.9283,  25.8422,  28.7623,  34.1087,  44.2230,\n            61.1873,  75.6124,  68.5497],\n          [ 16.2829,  19.0830,  22.1518,  25.6970,  29.8226,  34.5394,  40.6541,\n            51.2000,  76.6796, 115.6187],\n          [ 15.1046,  18.4827,  23.0270,  29.1324,  36.2404,  41.8678,  45.0167,\n            53.1469, 113.8960, 109.3406],\n          [ 14.7051,  19.0083,  25.9813,  37.9542,  56.4574,  70.1683,  63.0971,\n            55.3029,  70.5520, 107.4195],\n          [ 14.4812,  19.7270,  29.7652,  53.2629, 122.2569, 236.9708, 126.6235,\n            63.1753,  47.0361,  41.7441],\n          [ 13.9640,  19.5658,  31.1369,  63.2214, 233.5922, 234.7287, 236.2390,\n            68.6564,  39.1984,  28.9537]]]])\ntorch.Size([1, 10, 1, 1])\ntorch.Size([1, 10])\nPRED:  tensor([[5323402.]], grad_fn=<AddmmBackward>)\n\n\n[[195], [226], [87], [183]]\ntensor([2., 8., 9., 5., 6., 9., 0., 1., 8., 0., 4., 3., 0., 0., 4., 6.],\n       grad_fn=<ClampBackward>)\ntensor([[[[188.7433, 189.8791, 191.4096,  56.3413,  34.2657,  30.9007,  36.3844,\n            48.9172,  57.3472,  46.6059],\n          [ 97.8601, 190.7025, 101.0418,  48.8498,  34.9875,  35.9179,  52.0742,\n           108.7695, 205.1021, 106.6204],\n          [ 43.5978,  54.2940,  47.2517,  36.6122,  32.9064,  38.1477,  63.0603,\n           208.1891, 207.4670, 206.6056],\n          [ 25.9716,  29.7713,  30.0925,  29.1603,  30.3042,  36.5778,  55.3238,\n           113.9090, 211.8774, 114.0197],\n          [ 19.1759,  21.9084,  23.9283,  25.8422,  28.7623,  34.1087,  44.2230,\n            61.1873,  75.6124,  68.5497],\n          [ 16.2829,  19.0830,  22.1518,  25.6970,  29.8226,  34.5394,  40.6541,\n            51.2000,  76.6796, 115.6187],\n          [ 15.1046,  18.4827,  23.0270,  29.1324,  36.2404,  41.8678,  45.0167,\n            53.1469, 113.8960, 109.3406],\n          [ 14.7051,  19.0083,  25.9813,  37.9542,  56.4574,  70.1683,  63.0971,\n            55.3029,  70.5520, 107.4195],\n          [ 14.4812,  19.7270,  29.7652,  53.2629, 122.2569, 236.9708, 126.6235,\n            63.1753,  47.0361,  41.7441],\n          [ 13.9640,  19.5658,  31.1369,  63.2214, 233.5922, 234.7287, 236.2390,\n            68.6564,  39.1984,  28.9537]]]])\ntorch.Size([1, 10, 1, 1])\ntorch.Size([1, 10])\nPRED:  tensor([[5.2473e+18]], grad_fn=<AddmmBackward>)\n\n\n[[195], [226], [87], [183]]\ntensor([2., 8., 9., 5., 6., 9., 0., 1., 8., 0., 4., 3., 0., 0., 4., 6.],\n       grad_fn=<ClampBackward>)\ntensor([[[[188.7433, 189.8791, 191.4096,  56.3413,  34.2657,  30.9007,  36.3844,\n            48.9172,  57.3472,  46.6059],\n          [ 97.8601, 190.7025, 101.0418,  48.8498,  34.9875,  35.9179,  52.0742,\n           108.7695, 205.1021, 106.6204],\n          [ 43.5978,  54.2940,  47.2517,  36.6122,  32.9064,  38.1477,  63.0603,\n           208.1891, 207.4670, 206.6056],\n          [ 25.9716,  29.7713,  30.0925,  29.1603,  30.3042,  36.5778,  55.3238,\n           113.9090, 211.8774, 114.0197],\n          [ 19.1759,  21.9084,  23.9283,  25.8422,  28.7623,  34.1087,  44.2230,\n            61.1873,  75.6124,  68.5497],\n          [ 16.2829,  19.0830,  22.1518,  25.6970,  29.8226,  34.5394,  40.6541,\n            51.2000,  76.6796, 115.6187],\n          [ 15.1046,  18.4827,  23.0270,  29.1324,  36.2404,  41.8678,  45.0167,\n            53.1469, 113.8960, 109.3406],\n          [ 14.7051,  19.0083,  25.9813,  37.9542,  56.4574,  70.1683,  63.0971,\n            55.3029,  70.5520, 107.4195],\n          [ 14.4812,  19.7270,  29.7652,  53.2629, 122.2569, 236.9708, 126.6235,\n            63.1753,  47.0361,  41.7441],\n          [ 13.9640,  19.5658,  31.1369,  63.2214, 233.5922, 234.7287, 236.2390,\n            68.6564,  39.1984,  28.9537]]]])\ntorch.Size([1, 10, 1, 1])\ntorch.Size([1, 10])\nPRED:  tensor([[inf]], grad_fn=<AddmmBackward>)\n\n\n"
     ]
    }
   ],
   "source": [
    "x = make_example_data(14, size=4)\n",
    "y = torch.tensor([1], dtype=torch.float32)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(5):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    print(\"PRED: \", y_pred)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "x = SocialSig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nMake a layer that has a x trainabale params as the coordinates\\n    -- THIS LAYER NEEDS TO HAVE A FORWARD PASS\\n    (https://discuss.pytorch.org/t/pytorch-equivalent-of-keras/29412)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 666
    }
   ],
   "source": [
    "'''\n",
    "Make a layer that has a x trainabale params as the coordinates\n",
    "    -- THIS LAYER NEEDS TO HAVE A FORWARD PASS\n",
    "    (https://discuss.pytorch.org/t/pytorch-equivalent-of-keras/29412)\n",
    "'''\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('data442': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8e3a21d38ab9816cf2a4fb5b70910b2de32092d7fedca6365d5651d786256744"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}