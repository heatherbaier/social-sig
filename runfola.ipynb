{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import openturns as ot\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = [14, 12, 13, 12]\n",
    "# outDim = [10,10]\n",
    "\n",
    "# test = SocialSig(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialSig(torch.nn.Module):\n",
    "    '''\n",
    "    Class to create the social signature image\n",
    "    '''\n",
    "    def __init__(self, coords):\n",
    "        '''\n",
    "        Basic steps for class:\n",
    "            1. Randomly initalize 'weights' which I believe are actually the coords of the points\n",
    "            2. Train kriggin model to interpolate points in between\n",
    "            3. Predict what the points would be within a 224x224 matrix and output the resulting matrix\n",
    "        '''\n",
    "        super(SocialSig, self).__init__()\n",
    "        self.outDim = [10,10]\n",
    "#         self.coords = torch.nn.Parameter(torch.tensor(random.choices(range(0,outDim[1]), k=16), dtype=torch.float32)) \n",
    "        # self.register_parameter(name = 'coords', param = self.coords)\n",
    "\n",
    "    def forward(self, input, coords):\n",
    "        self.X = input\n",
    "        self.grid = self.__make_blank_coord_grid()\n",
    "        tensorRet = self.IDW(coords)\n",
    "        return tensorRet\n",
    "       \n",
    "    \n",
    "    def IDW(self, coords):\n",
    "        '''\n",
    "        Train the IDW model to predict all of the points that are between known points\n",
    "        '''\n",
    "        coords = torch.clamp(coords, min=0, max=self.outDim[1])\n",
    "        \n",
    "        for cell in range(0, len(self.grid)):\n",
    "            weightedVals = []\n",
    "            for column in range(0, len(X)):\n",
    "                xCoordLookup = column * 2\n",
    "                yCoordLookup = xCoordLookup + 1\n",
    "            \n",
    "\n",
    "                measurementCellValue = self.X[column]\n",
    "\n",
    "                estCellX = self.grid[cell][0]\n",
    "                estCellY = self.grid[cell][1]\n",
    "\n",
    "                measureCellX = coords[xCoordLookup]\n",
    "                measureCellY = coords[yCoordLookup]\n",
    "\n",
    "                A2 = abs(estCellX - measureCellX)**2\n",
    "                B2 = abs(estCellY - measureCellY)**2\n",
    "                C2 = math.sqrt(A2+B2) \n",
    "                if(C2 == 0):\n",
    "                    C2 = 1\n",
    "                \n",
    "                weightedVals.append(measurementCellValue[0] * (1/(C2**2)))\n",
    "            self.grid[cell] = sum(weightedVals)\n",
    "        numpyGrid = torch.from_numpy(np.reshape(np.array(self.grid), (1,1,self.outDim[0],self.outDim[1])))\n",
    "        tensorGrid = torch.tensor(numpyGrid, dtype=torch.float)\n",
    "        # print(self.coords)\n",
    "\n",
    "        return tensorGrid \n",
    "        \n",
    "    def __make_blank_coord_grid(self):\n",
    "        '''\n",
    "        Make a blank coordinate grid to fill in with real data later\n",
    "        '''\n",
    "        return [[x,y] for x in range(0, outDim[0]) for y in range(0,outDim[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[226], [102], [41], [193]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS IS WHAT YOU WILL USE TO CREATE YOUR INPUT LAYER--THIS SHOULD BE A LIST WITH 81 ELEMENTS (THE ROW IN THE DF)\n",
    "def make_example_data(seed, size):\n",
    "    '''\n",
    "    Create the training dataset by randomly assigning coordiantes to the known data (i.e the data within a row)\n",
    "    '''\n",
    "    points = [[i] for i in list(np.random.randint(low = 0, high = 255, size = size).flatten())]\n",
    "    return points\n",
    "\n",
    "\n",
    "make_example_data(14, size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "#         self.coords = torch.nn.Parameter(torch.tensor(random.choices(range(0,outDim[1]), k=16), dtype=torch.float32)) \n",
    "        self.coords = torch.nn.Parameter(torch.empty(outDim[1]))\n",
    "        nn.init.uniform_(self.coords, a = 0, b = 8)   \n",
    "\n",
    "        self.SocialSig = SocialSig(self.coords)\n",
    "        self.conv1 = nn.Conv2d(1, 10, 10, 1)\n",
    "        self.linear1 = torch.nn.Linear(10, 1)\n",
    "        self.linear2 = torch.nn.Linear(2, 1)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5, requires_grad=True))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print(input)\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        print(self.coords)\n",
    "        sig = self.SocialSig(input, self.coords)#.forward(input, self.coords)\n",
    "#         print(sig)\n",
    "        sig = self.conv1(sig)\n",
    "#         print(sig.size()) #1,10,1,1\n",
    "        sig = torch.flatten(sig, 1)\n",
    "#         print(sig.size())#1,10\n",
    "        sig = self.linear1(sig)\n",
    "\n",
    "        return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above\n",
    "x = make_example_data(14, size=4)\n",
    "model = TwoLayerNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.named_parameters():\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([3.3548, 4.3685, 3.1820, 0.8569, 7.7401, 1.9282, 7.5094, 1.6299, 4.6097,\n",
      "        7.3130], requires_grad=True)\n",
      "PRED:  tensor([[103.8194]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "Parameter containing:\n",
      "tensor([3.3548, 4.3685, 3.1820, 0.8569, 7.7401, 1.9282, 7.5094, 1.6299, 4.6097,\n",
      "        7.3130], requires_grad=True)\n",
      "PRED:  tensor([[225642.1875]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "Parameter containing:\n",
      "tensor([3.3548, 4.3685, 3.1820, 0.8569, 7.7401, 1.9282, 7.5094, 1.6299, 4.6097,\n",
      "        7.3130], requires_grad=True)\n",
      "PRED:  tensor([[3.2169e+15]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "Parameter containing:\n",
      "tensor([3.3548, 4.3685, 3.1820, 0.8569, 7.7401, 1.9282, 7.5094, 1.6299, 4.6097,\n",
      "        7.3130], requires_grad=True)\n",
      "PRED:  tensor([[nan]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "Parameter containing:\n",
      "tensor([3.3548, 4.3685, 3.1820, 0.8569, 7.7401, 1.9282, 7.5094, 1.6299, 4.6097,\n",
      "        7.3130], requires_grad=True)\n",
      "PRED:  tensor([[nan]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heatherbaier/anaconda/envs/caoe/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "x = make_example_data(14, size=4)\n",
    "y = torch.tensor([1], dtype=torch.float32)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(5):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    print(\"PRED: \", y_pred)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[7.0065e-45, 0.0000e+00]]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.3727, -0.6900]]], requires_grad=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hey = nn.Parameter(torch.Tensor(1, 1, 2))\n",
    "b = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "print(hey)\n",
    "# print(b)\n",
    "\n",
    "# init params\n",
    "nn.init.xavier_uniform_(hey)\n",
    "# b.data.uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported nonlinearity Parameter containing:\ntensor([8., 5., 5., 6., 5., 2., 6., 5., 6., 1., 3., 4., 2., 6., 3., 8.],\n       requires_grad=True)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-45dc288e0df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# nn.init.xavier_uniform_(hey)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/caoe/lib/python3.6/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mcalculate_gain\u001b[0;34m(nonlinearity, param)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m3.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m4\u001b[0m  \u001b[0;31m# Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsupported nonlinearity {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonlinearity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported nonlinearity Parameter containing:\ntensor([8., 5., 5., 6., 5., 2., 6., 5., 6., 1., 3., 4., 2., 6., 3., 8.],\n       requires_grad=True)"
     ]
    }
   ],
   "source": [
    "hey = torch.nn.Parameter(torch.tensor(random.choices(range(0,outDim[1]), k=16), dtype=torch.float32)) \n",
    "\n",
    "# nn.init.xavier_uniform_(hey)\n",
    "\n",
    "torch.nn.init.calculate_gain(hey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([4.1101, 1.6600, 2.3353, 3.6779, 2.9621, 4.4281, 7.5158, 6.6865, 1.7108,\n",
       "        1.7685], requires_grad=True)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.nn.Parameter(torch.empty(10))\n",
    "nn.init.uniform_(w, a = 0, b = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alpha', Parameter containing:\n",
      "tensor(0.5000, requires_grad=True))\n",
      "('coords', Parameter containing:\n",
      "tensor([6., 0., 4., 3., 7., 7., 7., 8., 4., 7., 3., 1., 8., 0., 6., 0.],\n",
      "       requires_grad=True))\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]],\n",
      "\n",
      "\n",
      "        [[[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]],\n",
      "\n",
      "\n",
      "        [[[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]],\n",
      "\n",
      "\n",
      "        [[[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]]],\n",
      "\n",
      "\n",
      "        [[[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]],\n",
      "\n",
      "\n",
      "        [[[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]]],\n",
      "\n",
      "\n",
      "        [[[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]],\n",
      "\n",
      "\n",
      "        [[[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]]],\n",
      "\n",
      "\n",
      "        [[[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]]],\n",
      "\n",
      "\n",
      "        [[[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
      "          [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]]]],\n",
      "       requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([-inf, -inf, -inf, inf, -inf, inf, -inf, inf, inf, inf],\n",
      "       requires_grad=True))\n",
      "('linear1.weight', Parameter containing:\n",
      "tensor([[-inf, -inf, -inf, inf, -inf, inf, -inf, inf, inf, inf]],\n",
      "       requires_grad=True))\n",
      "('linear1.bias', Parameter containing:\n",
      "tensor([-inf], requires_grad=True))\n",
      "('linear2.weight', Parameter containing:\n",
      "tensor([[0.0372, 0.4206]], requires_grad=True))\n",
      "('linear2.bias', Parameter containing:\n",
      "tensor([-0.4777], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "x = SocialSig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMake a layer that has a x trainabale params as the coordinates\\n    -- THIS LAYER NEEDS TO HAVE A FORWARD PASS\\n    (https://discuss.pytorch.org/t/pytorch-equivalent-of-keras/29412)\\n'"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Make a layer that has a x trainabale params as the coordinates\n",
    "    -- THIS LAYER NEEDS TO HAVE A FORWARD PASS\n",
    "    (https://discuss.pytorch.org/t/pytorch-equivalent-of-keras/29412)\n",
    "'''\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caoe",
   "language": "python",
   "name": "caoe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
